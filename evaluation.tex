\chapter{Benchmarking and Performance Experiments}
\label{chapter:experiments}

The goal of this Chapter is to quantify the performance of various common operations performed by users in Tribler. We will perform a number of experiments and for each experiment, we will present and discuss the observed results.

\section{Experiments environment}
The experiments performed in this Chapter using the Tribler core are executed on a virtual private server. An important condition is to stay as close as possible to the specifications of a machine that an actual user could be using. The virtual server has 8GB of memory and 1 processor with 4 cores (where each core has a clock speed of 2.5GHz). The used operation system is Ubuntu 15.10. The experiments are not executed in an isolated, artificial environment but instead in the wild, using the deployed Dispersy network. While the obtained results may be different between users, this setup can be used to get insight in the performance of Tribler from a user's perspective.\\\\
If not stated otherwise, the default values of the Tribler configuration file are used. These default values can be found in the `defaults.py` file in the source code directory of Tribler\footnote{https://github.com/Tribler/tribler/blob/devel/Tribler/Core/defaults.py}. In this configuration file, all communities, except for \emph{BarterCast}, are joined.\\\\
Some of the experiments are specified by the usage of a scenario file. In such a scenario file each line specifies a specific command of a peer at a specific point in time into the experiment. Our framework to run the experiments, Gumby, contains code to read scenario files, interprets the commands to be executed and to schedule them on the reactor thread. Several utility methods have been implemented to gather and write statistics to files in a processable and readable format that can be parsed by visualization tools such as \emph{R}. The usage of scenario files is already adopted in various Dispersy experiments, mainly in our \emph{AllChannel} experiment that runs on the DAS5 supercomputer. We extended the usability of this approach to run a Tribler client and we improved the framework by adding various commands to support operations as conducted in the performed experiments in this Section. The flexibility of these scenario files gives the next generation of Tribler developers a robust framework to use when conducting performance analysis, scientific research and benchmarking\todo{tabel met commands?}.

\section{Profiling Tribler on low-end devices}
The addition of a REST API allows developers an option to run and control Tribler remotely using the HTTP API. For instance, one can run Tribler on a low-end, cheap devices such as a Raspberry Pi. Android is another example of a device that can run Tribler and during the last years, various research have been conducted to explore the possibilities of Tribler on Android devices\cite{sabee2014tribler}\cite{de2014android}. Executing and profiling Tribler on a low-end device with limited resources can yield much information about bottlenecks that might not be directly visible when running Tribler on a desktop or supercomputer.\\\\
The experiments described in this Section are all executed on a Raspberry Pi, third generation with 1GB LPDDR2 ram, 4Ã— ARM Cortex-A53, 1.2GHz CPU and 16GB storage on a microSD card. The used operating system is Raspbian, a system specifically designed for the Raspberry Pi and derived from the desktop Debian operating system.\\\\
Regular usage of Tribler on the Raspberry Pi using the REST API has us suspected that the Raspberry Pi is under heavy load when running Tribler. Monitoring the process for a while using the \emph{top} tool, reveals that the CPU usage is often around 100\%, completely filling up one CPU core. To get a detailed breakdown of execution time per method in the code base, the Yappi profiler has been used to gather statistics about the runtime of methods. This profiler has been integrated in the \emph{twistd} plugin and can be started together with Tribler by passing a flag. The output generated by the profiler is a \emph{callgrind} file that should be loaded and analysed by third party software. The breakdown of a 20-minute run is visible in Figure \ref{fig:yappi_breakdown}. This breakdown is generated using \emph{QCacheGrind}, a \emph{callgrind} file visualizer. In this experiment, we start with a clean state directory which is equivalent to the first boot of Tribler.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.9\columnwidth]{images/experiments/yappi_breakdown}
	\caption{The breakdown of a 20-minute run of Tribler on the Raspberry Pi.}
	\label{fig:yappi_breakdown}
\end{figure}

The file created by the Yappi profiler provides a detailed overview of the execution time of methods and can be used as a tool to detect performance bottlenecks in the system. Referring to Figure \ref{fig:yappi_breakdown}, the column \emph{Incl.} denotes the inclusive cost of the function, in other words, the execution time of function itself and all the functions it calls. The column \emph{self} denotes only the execution time of the function itself, without considering callees. The other columns are self-explanatory and can be used as reference to the location of the respective function in the source code.\\\\
If we analyse the breakdown, it is clear that Dispersy has a big impact on the performance of Tribler when running on the Raspberry Pi. The \emph{ecdsa\_verify} method (second method from the bottom) is dominating the runtime of Tribler: 45.81\% of the time Tribler is running, is spent inside this method. This specific method verifies the signature of an incoming Dispersy message and is invoked every time a signed message is received. Disabling cryptographic verification of incoming messages should improve the situation, however, this is a trade-off between security and performance: by not verifying incoming messages, fake messages by an adversary can be forged and are accepted in such as system.\\\\
To verify whether the responsiveness of Tribler improves when we disable cryptographic verification of incoming messages, we measure the CPU usage of two runs. Both runs start with a non-existing Tribler state directory and have a duration of ten minutes. In the first run, we are using the default configuration of Tribler, like in most of the other experiments described in this Chapter. In the second run, we disable verification of incoming messages. The CPU utilization over time of the two runs are displayed in Figure \ref{fig:raspi_cpu_usage}.\\\\

\begin{figure}[!h]
	\centering
	\includegraphics[width=1.0\columnwidth]{images/experiments/raspi_cpu_usage}
	\caption{The CPU utilization of one core on a Raspberry Pi device when running Tribler with different cryptographic policies.}
	\label{fig:raspi_cpu_usage}
\end{figure}

In Figure \ref{fig:raspi_cpu_usage}, some occurrences are visible where the CPU usage appears to be slightly over 100\%. This is explained by the fact that some of the underlying code is designed to run on multiple processors. While the threading model of Tribler is limited to a single core, the Python interpreter might execute code on additional cores to improve performance. In the run where we enable all components of the system, the CPU usage is often 100\%. When verification of Dispersy messages is disabled, we observe a somewhat lower CPU usage but overall, this utilization is still high. Unfortunately, disabling incoming message verification is clearly not enough to guarantee a more usable and responsive system.\\\\
To detect other performance bottlenecks, we sort the report generated by the Yappi profiler on the \emph{Self} column to get insight in methods that are taking a long time to complete. This is visible in Figure \ref{fig:yappi_breakdown_self}. An interesting observation here is that the Python built-in \emph{all} method takes up a significant amount of time (6.13\% of the runtime). The \emph{all} method takes an iterable object and returns \emph{true} if all objects of this collections are true. Both the \emph{all} method and \emph{zip} method (also visible in Figure \ref{fig:yappi_breakdown_self}) is used in the \emph{\_resume\_delayed} method, indicating that this method might causing performance issues. Since further analysis of this method requires more knowledge of Dispersy, optimization of this bottleneck is considered future work and described in GitHub issue 505\footnote{https://github.com/Tribler/dispersy/issues/505}.\\\\

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.9\columnwidth]{images/experiments/yappi_breakdown_self}
	\caption{The breakdown of a 20-minute run of Tribler on the Raspberry Pi, sorted on the \emph{Self} column.}
	\label{fig:yappi_breakdown_self}
\end{figure}

In this Section, we demonstrated how adequate usage the Yappi profiler can lead to the detection of bottlenecks in the system. Integration in the twistd plugin makes it convenient for developers to run and analyse Tribler sessions under different circumstances.

\section{Performance of the REST API}
The performance of the REST API is directly influencing the user experience. If the response times of API calls is high, users have to wait longer before their data is available and visible. This is why we wish to make the API serve requests as fast as possible. The experiments to assess the performance of the API will particularly focus on latency of requests, however, some other statistics will be considered such as average request time, standard deviation of the response time and throughput to get more insights in the performance of the REST API.\\\\
The Apache JMeter application\cite{jmeter2010apache} is used to perform HTTP requests to Tribler and to gather and analyse performance statistics. The application allows to simulate a realistic user load, however, in this experiment we will limit the load to one user that performs a request every fixed interval. This request will be targeted to a specific endpoint in the API: \emph{/channels/discovered}. This exact call happens when users are pressing the \emph{discover} menu button in the new Qt GUI and the response of the request contains a list of all discovered that Tribler has discovered. As a consequence, the returned response can be rather large (in our experiments, the average response size is around 613KB). When the request is processed, a database lookup is performed to fetch all channels that are stored in the local database.\\\\
We perform various experiments with different intervals between requests made and a fixed amount of 500 requests. First, we perform the experiment with one request every second and we expect that the system should easily be able to hand this load and serve these requests in a timely matter. Next, the frequency of requests is increased to respectively 2, 5 and 10 requests per second. Each experiment is started around five seconds after Tribler has started where we are using a pre-filled database with around 100.000 discovered torrents and 1.200 channels. A summary of the results of these experiments are visible in Table \ref{table:performance-api-results}.\\\\

\begin{table}[]
	\centering
	\begin{tabular}{|l|l|l|l|l|l|l|}
		\hline
		\textbf{Requests/sec} & \textbf{Avg. (ms)} & \textbf{Std. dev (ms)} & \textbf{Median (ms)} & \textbf{Min. (ms)} & \textbf{Max. (ms)} & \textbf{KB/S} \\ \hline
		\emph{1} & 241 & 476.34 & 76 & 56 & 4246 & 585.58\\ \hline
		\emph{2} & 170 & 327.86 & 68 & 58 & 3394 & 1127.04\\ \hline
		\emph{5} & 123 & 210.23 & 60 & 52 & 2082 & 2538.36\\ \hline
		\emph{10} & 115 & 238.72 & 60 & 50 & 2450 & 4120.70\\ \hline
		\emph{15} & 182 & 497.61 & 68 & 52 & 4937 & 3296.45\\ \hline
	\end{tabular}
	\caption{A summary of the experimental results when measuring the performance of the REST API.}
	\label{table:performance-api-results}
\end{table}

The most interesting observation from this table is that it appears that requests are served faster if we are performing requests at a faster rate, indicating that Tribler is able to handle the incoming requests well. This is surprising since one would expect the results to be the other way around: when the frequency of requests is increased, the average request time increases. The observed result is most likely due to caching of data which might be performed by the underlying database engine.\\\\
The standard deviation of the request times is rather large compared to the average request time. We suspect that this might be explained the fact that Tribler is performing many different operations that are influencing the request times. In particular, we think that the reactor thread is busy with processing other calls that have higher priority, causing the API calls to be scheduled later. To verify this, we will run the experiment again where we enable Dispersy which is responsible for many calls in the reactor (as described in Section x\todo{verwijzing}), where five requests per second and 500 requests in total are performed. The results of this experiment are illustrated in Figure \ref{fig:api-performance}. On the left, the response times of the performed requests with a full Tribler session is displayed whereas on the right, we display the response times of the run where we disable Dispersy. Note the different scale on the vertical axis. The average request time of the plot on the right is 48 ms, significantly lower than the average of the response times when Dispersy is enabled: 123ms. While both graphs are producing a spiky pattern, the standard deviation of the right plot is 5.73. We conclude that the variation in response times is lower in the right plot and that most likely DIspersy is producing much work for the reactor thread, introducing latency when performing API requests.\\\\
We possibly identified a key issue here: the latency in the Twisted reactor is high, causing the processing of incoming requests to be delayed. This does not only hold for the REST API, we can conclude the same for the tunnel community.\\\\
Table \ref{table:performance-api-results} gives us another insight, namely that we it appears that the bandwidth is reducing as the number of requests per second increases. This is in particular visible if we plot the theoretical maximum bandwidth against the observed bandwidth during the experiments, see Figure \ref{fig:api-bandwidth-performance}. In this Figure, we presented both the obtained bandwidth for a full Tribler session and a session where Dispersy has been disabled. We assume that each request contains 613KB response data. The theoretical bandwidth is calculated as $ b = 613* n $ where $ n $ is the number of requests per second. This gives $ b $, the bandwidth per second. In practice, we will never reach the theoretical bandwidth since some time is required to create the connection and to process the response data in Tribler. We still notice that the obtained bandwidth is somewhat becoming constant, indicating that the bandwidth we can obtain is limited. This picture clearly shows the impact of a running Dispersy on the bandwidth. Whereas we almost obtain the theoretical output when we disable Dispersy, the gap between the theoretical and observed bandwidth becomes bigger in the run where we use a full session\todo{overal horizontal axis/vertical axis uitleggen}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1.0\columnwidth]{images/experiments/api_bandwidth_performance}
	\caption{The maximum theoretical bandwidth compared to the obtained bandwidth (using a full Tribler session and disabled Dispersy) in the experiments.}
	\label{fig:api-bandwidth-performance}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=1.0\columnwidth]{images/experiments/request_times_comparison}
	\caption{Test.}
	\label{fig:api-performance}
\end{figure}

\section{Start-up experience}
The very first interaction that users have with Tribler, is the process of starting. During the boot process, various operations are performed:
\begin{itemize}
	\item The connection to the sqlite database is opened and initialized. If this database does not exist, it will be created.
	\item Dispersy is started and the enabled communities are loaded.
	\item Various Tribler components are created, including the video streaming server, the REST API, the remote torrent handler and the \emph{leveldb} store.
\end{itemize}
The start-up process of the Tribler core happens sequentially and no parallel operations are in place to speed up the process. Depending on the number of enabled components, the start-up time might vary.\\\\
To analyse the start-up time, Tribler is started 30 times. In half of the runs, the software is started for the first time, with no prior existing state directory. In these runs, a state directory is created and the required files are initialized. In the other half of the runs, a database with a little over 100.000 torrents is used. This database is the result of running Tribler idle for several hours, after subscribing to some popular channels. Moreover, a filled Dispersy database is used for the second half of the runs. In both scenarios, there are no downloads running. The experiment starts when the \emph{start} method of the \emph{Session} object is called and ends when the notification that Tribler has started, is observed. The results are displayed in Figure \ref{fig:startup_experiment}.\\\\
It is clear that magnitudes of the Tribler and Dispersy databases have impact on the time for Tribler to completely start. However, this impact is relatively minor since Tribler still starts within a second. We think that this statistic justifies removal of the splash screen that is shown in the old user interface. The relatively short time the splash screen would be visible in the new interface is so short that users would not even be able to read and interpret the content of the splash screen.\\\\
In both plots, some outliers are present. Further analysis learns us that these variations are caused by the loading process of the Dispersy communities, however, further investigation of the initialization part of Dispersy is considered future work.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1.0\columnwidth]{images/experiments/startup}
	\caption{The boot time of Tribler from a clean and pre-filled state using the code base in November '15 and July '16.}
	\label{fig:startup_experiment}
\end{figure}

\section{Remote Content Search}
We wish to serve relevant information to users as fast as possible. To help users search for relevant content, a remote keyword search has been implemented. Matching search results can either be torrents or channels. Channel results are fetched by a query in the \emph{AllChannel} community whereas torrent results are retrieved by a query in the \emph{search} community.\\\\
To verify the speed of the remote torrent search, various experiments are conducted. We are using a list of 100 terms that users might be searching for. This list of search terms can be found in Appendix x\todo{Add this}. Each query is executed when there are at least 20 connected peers in the \emph{SearchCommunity}. The timeout period of the remote search is 60 seconds. This experiment has a particular focus on two statistics: the time until the first remote torrent search results comes in and the turnaround time of the search request. We should note that users performing a remote search might see results earlier since in parallel, a  search query in the local database is performed. The results are visible in Figure \ref{fig:remote_search_first_result} and \ref{fig:remote_search_all_result}.\todo{nitin graph}\\\\
Overall, the remote torrent search as implemented in Tribler is very fast and performs well. On average, 61 search results are returned for each query and the first incoming torrent result takes 0.26 seconds to arrive. As we see in Figure \ref{fig:remote_search_first_result}, over 90\% of the first remote search results are available to the user within a second. During our experiment, we always have the first incoming torrent result within 3.5 seconds.\\\\
The other graphs shows the turnaround time of the request. On average, it takes 2.1 seconds for all torrent search results to arrive. From Figure \ref{fig:remote_search_all_result}, we see that in over 90\% of the search queries, we have all results within 10 seconds.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1.0\columnwidth]{images/experiments/cdf_remote_search}
	\caption{The performance of remote content search, expressed in the time until the first response and time until last response.}
	\label{fig:remote_search}
\end{figure}

\section{Local Content Search}
In the previous Section, we demonstrated and elaborated the performance of the remote content search algorithm. Now, we will shift the focus to performance measurements of a local content search, which is considered more trivial than the remote search counterpart where network communication is required. In particular, our goal is to quantify the performance gain or loss when switching to the new relevance ranking algorithm, as described in Section x\todo{schrijven}.\\\\
The setup of the experiment is as follows: a database with a little over 100.000 torrents is used. Every second, we perform a local \emph{torrent} search and we do this for 1.000 random keywords that are guaranteed to match at least one torrent in database. We will measure both the time for the database lookup and the time it takes for the data to be post-processed after being fetched from the database. This post-processing step involves the assignment of a weight to each of the results so the list can be sorted on relevance. This experiment is performed for the old ranking algorithm that uses the Full Text Search 3 (FTS3) engine and the new ranking algorithm that uses the more recent FTS4 engine. According to the SQLite documentation, FTS3 and FTS4 are nearly identical, however, FTS4 contains an optimization where results are returned faster when performing searches that are common in the database. The results of the experiments are visible in Figure \ref{fig:local-search-fts3-fts4}, displayed in an ECDF.\\

\begin{figure}[h!]
	\centering
	\includegraphics[width=1.0\columnwidth]{images/experiments/local_search_fts3_fts4}
	\caption{A comparison of the performance of local keyword searches between the old ranking algorithm with the FTS3 engine and the new ranking algorithm utilizing the FTS4 engine.}
	\label{fig:local-search-fts3-fts4}
\end{figure}

Local content search is very fast, delivering results in several milliseconds and low priority should be given to performance engineering on the local content search engine. We see that the two lines in the FTS3 and FTS4 graphs have moved closer to each other which means that the speed of the post-processing of torrent results have increased. This is in line with our expectations since the new relevance ranking algorithm should be less computationally expensive than the old one. In addition, the new algorithm takes less factors in considering, for instance, the swarm health of the torrent. The performance increase from FTS3 to FTS4 is visible but not significant.\\\\
In 2009, Nitin et al. performed the same experiment where they used 50.000 torrents in their database. The generated ECDF is displayed in Figure \ref{fig:local-search-nitin}. We notice that the current performance of local search in our experiment is dramatically better than the performance obtained during the 2009 experiment. This can be explained by the fact that Tribler used a custom inverted index implementation when the experiment in 2009 was conducted. An inverted index is an index data structure where a mapping is stored from words to their location in the database and is used on a large scale by search engines, including the FTS engine of SQLite. By utilizing this mapping when performing a full text search, we can get results in constant time. However, there is a slight overhead for maintaining and building the inverted index when new entries are added to the database, also impacting the size of the database disk file. The built-in FTS engine of SQLite is optimized to a large extent and clearly offers a higher performance than a custom implementation.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1.0\columnwidth]{images/experiments/nitin_local_search}
	\caption{The performance of a local database query as verified by Nitin et al. in 2009.}
	\label{fig:local-search-nitin}
\end{figure}

\section{Video streaming}
The embedded video player in Tribler allows users to watch a video that is being downloaded. Available since Tribler 4.0, the video playback did not evolve much. The video player is based on VLC and offers support for seeking so the user can jump randomly to a specified offset in the video. Video downloads have a special Video On Demand (VOD) mode which means that the libtorrent library piece picking mechanism uses a linear policy. In this mode, pieces are downloaded in a timely matter. When the user seeks to a position in the video, the prioritization of the pieces are modified, giving priority to pieces around the chosen seek position. Users also have the possibility to use an external video player that support playback of HTTP video streams.\\\\
The bytes are streamed to a VLC client using HTTP. When Tribler starts, a HTTP video server is started. This server supports HTTP range requests which means that a specific part of a video file can be queried by using the HTTP \emph{range} header. This is useful when the user performs a seeking operation since only a specific part of the file has to be returned in the HTTP response. If some pieces are not available, the video server will wait until these bytes are downloaded and available before returning these bytes in the response.\\\\
To improve user experience, we wish to minimize the delay that users experience when performing a seek operation in the video player. The experiment performed in this Section, will measure the buffering delay. For this purpose, the well-seeded \emph{Big Buck Bunny}\footnote{https://peach.blender.org} movie will be downloaded. We will perform various HTTP range requests using the \emph{curl} command line tool. Every request, 10 megabyte of data will be requested and we will measure the total request time for each of these requests. The results are visible Table \ref{table:video_player_seek_times}.\\

\begin{table}[]
	\centering
	\label{table:video_player_seek_times}
	\begin{tabular}{|l|l|}
		\hline
		First byte               & Request time (sec) \\ \hline
		0                        & 11.6                  \\ \hline
		$ 1 * 10^9 $ & 64.4                  \\ \hline
		$ 2 * 10^9 $ & 64.6                  \\ \hline
		$ 3 * 10^9 $ & 65.9                   \\ \hline
		$ 4 * 10^9 $ & 100.6                   \\ \hline
		$ 5 * 10^9 $ & 115.6                   \\ \hline
		$ 6 * 10^9 $ & 115.8                  \\ \hline
		$ 7 * 10^9 $ & 12.2                  \\ \hline
		$ 8 * 10^9 $ & 66.6                   \\ \hline
		$ 9 * 10^9 $ & 52.4                   \\ \hline
	\end{tabular}
	\caption{My caption}
\end{table}

Theoretically, we would expect around the same request time for each range request, assuming that the availability of each piece is high. When performing a seek, the piece picking mechanism adjusts priorities and these prioritized pieces should start to download immediately. The experiments shows some serious flaws in this mechanism where it might take up to two minutes for data to be available. Further investigation of this issue learns us that the video player always tries to download the first 10\% of the video file, except the 7th run of the experiment, where the prioritizing seems to be correctly applied. Solving this bug is considered further work and described in more detail on Github issue 1234\todo{issue maken}.

\section{Content discovery}
Content discovery is one of the most important features of Tribler. By running Tribler idle for a while, content is synchronized with other peers using the Dispersy framework. When a user starts Tribler for the first time, there is no available content yet. In this Section, we will verify the content discovery speed after the first start. The experiment is structured as follows: we measure the interval from the completion of start procedure to the moment where the first content is discovered. This is done for both torrents and channels and repeated 15 times. The results of this experiment are visible in Figure \ref{fig:content_discovery_speed}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.6\columnwidth]{images/experiments/content_discovery_speed}
	\caption{Test.}
	\label{fig:content_discovery_speed}
\end{figure}

The delay of discovering the first channel is reasonably. In all runs, we have our first content available within 35 seconds. Discovery times of the first torrent is slightly slower and in all runs, the first torrent in a channel is discovered within 40 seconds. In the old user interface, users were presented with a blank screen with no feedback about new discovered content. In the new interface, the user is presented with a screen that informs the user that Tribler is discovering the first content. This screen is only shown the first time Tribler is started.

\section{Channel subscription}
When Tribler runs idle, not all content is discovered. The majority of content is discovered when users are subscribing to channel. When Tribler discovers a channel, users are presented with a preview of this channel. Internally, Tribler joins a \emph{PreviewChannelCommunity}, a community derived from the main \emph{ChannelCommunity}. I this preview, the amount of torrents to be collected is limited. The associated \emph{ChannelCommunity} is joined the moment the user subscribes to a channel, after which the full range of content is synchronized with the subscribed user. Removing the preview mechanism significantly increases the resource usage of the Tribler session since the amount of incoming messages to be decoded and verified will rise.\\\\
The experiment as described in this Section, will focus on the discovery speed of additional content after the user subscribes to a specific channel. For this experiment, the 20 most popular channels (with the most subscribers) are determined. A Tribler state directory has been installed that discovered channels but did not subscribe to any of them. Exactly 10 seconds after Tribler started, we subscribe to one of these channels and we measure the interval between subscriptions and discovery of the first additional torrent in this channel. Tribler is restarted between every run so we guarantee a clean state of the system. The results are visible in Figure \ref{fig:channel_subscription}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.9\columnwidth]{images/experiments/channel_subscription}
	\caption{Test.}
	\label{fig:channel_subscription}
\end{figure}

The average discovery time of additional torrents is 36.8 seconds, which is quite long, compared to the performance of remote search. The discovery times have a somehow high variation as can be seen in Figure \ref{fig:channel_subscription}. This can be explained by the fact that immediately after subscribing to a channel, new peers have to be found in the \emph{ChannelCommunity} that is joined after subscription. Moreover, some of the channels might have more torrents available. After subscription to a channel with a higher amount of torrents, it is more likely for content to be discovered faster.\\\\
To verify the impact of automatically subscribing to each channel, we perform a CPU usage measurement. In two idle runs of a Tribler sessions, both lasting for ten minutes, we measure the CPU usage every ten seconds. In the first run, a regular Tribler session is used where previews of discovered channels are enabled. In the second run, we bypass the preview of a discovered channel and immediately join the channel, synchronizing all available content. The results of this experiment are visible in Figure \ref{fig:channel_subscription_cpu}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1.0\columnwidth]{images/experiments/subscribe_cpu_experiment}
	\caption{Test.}
	\label{fig:channel_subscription_cpu}
\end{figure}

Whereas the CPU usage of the normal run is around 50\% on average, the CPU is busy when we enable auto-join of channels. This shows that it is infeasible to enable this auto-join feature. One might limit the number of fetched torrents, however, this requires a feedback mechanism where we should notify other peers to limit the amount of messages sent to the peer that is discovering content in the channel. Implementing of such as feature is considered future work.

\section{Torrent Availability and Lookup Performance}
A possible source of torrents is the Distributed Hash Table (DHT). The DHT provides primitives to query torrent files and peers, based on a specific infohash of a torrent. Querying the DHT for torrent files can be done by invoking the \emph{download\_torrentfile} in the \emph{Session} object. One should specify the callback to be invoked after the metainfo is successfully downloaded. In this Section, experiments will be conducted to get insights in the availability of torrent files and the performance of lookup operations in the DHT. This experiment is relevant since users that want to determine whether specific content is interesting or not, first might want to view metainfo of the torrent file. This metainfo should be available as soon as possible.\\\\
In the current user interface, the torrent file is fetched when the user single clicks on a torrent in the list of torrents, either when browsing through contents of a channel or after performing a remote keyword search. In addition, when executing a remote search, the first three top-results are pre-fetched since the user might be interested in them. For this experiment, a popular channel with over 5.000 torrents is taken and a subset of infohashes of torrents in this channel is calculated. Every 40 seconds, a DHT query is performed with one of the 1.000 random infohashes. The timeout period used in Tribler is 30 seconds, after which a failure callback is invoked and an error is displayed in the user interface. The results of this experiments are visible in Figure \ref{fig:metainfo_fetch}. Torrents that cannot be successfully resolved from the DHT, are assigned a value of 30 seconds in the graph.\\\\

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.9\columnwidth]{images/experiments/metainfo_fetch}
	\caption{Test.}
	\label{fig:metainfo_fetch}
\end{figure}

We immediately notice that the failure rate of DHT lookups is quite high: a little under 50\% of the lookup operations are timing out. This might be addressed to dead torrents (no peers in the DHT have this torrent available) or private torrents (torrents which information is not spread in the DHT). The amount of failures might be even higher in a less popular channel since the content in these channels are less seeded. We should note that the DHT is not the only source of torrents in Tribler. When performing a remote query search, each incoming torrent result might have candidates attached that possibly have this torrent in their database. When clicking on a torrent result in the list of search results, not only the DHT is queried, each of the candidates of the torrent result is queried in parallel using the Trivial File Transfer Protocol (TFTP)\cite{sollins1992tftp}. TFTP is a simplified version of the more sophisticated File Transfer Protocol which is commonly used to transfer files between web servers. Tribler contains a dedicated Python package with a TFTP implementation. Unfortunately, the approach of fetching metainfo of torrents from other peers is only usable when searching for torrents. Caching and exchanging torrent candidates is not successful since the availability of candidates later cannot be guaranteed.\\\\
The average lookup time of torrents that are successfully fetched from the DHT is 5.81 seconds which is reasonably fast. Additionally, Figure \ref{fig:metainfo_fetch} shows that a little over 90\% of the successfully fetched torrents are retrieved within 10 seconds.\\\\
To improve performance of metainfo lookup, dead torrents should be handled correctly. One possible solution might be an implementation of a periodical check for each incoming torrent. By limiting the number of outstanding DHT requests, this approach does not create require much additional resources. To further improve performance, the result of DHT lookups might be disseminated to remote peers in the network. Torrents that are not successfully fetched from the DHT, could be hidden automatically in the user interface. The downside of this approach is that it might not give a realistic view of the availability of a torrent since their might be candidates which have a copy of this torrent available.